# Experiment Configuration
experiment:
  name: "gpt2_training_v1"          # Unique name for this experiment
  version: "1.0.0"                  # Version of the experiment
  seed: 42                          # Random seed for reproducibility

# Data Configuration
data:
  path: "data/input.txt"            # Path to raw input data
  version: "2025-03-11_v1"          # Data version (tracked with DVC)
  split_ratio: 0.9                  # Train/validation split ratio
  block_size: 1024                  # Maximum sequence length

# Model Configuration
model:
  type: "GPT2"                      # Model class name
  d_model: 768                      # Embedding dimension
  block_size: 32                    # Block size
  n_heads: 12                       # Number of attention heads
  n_layers: 12                      # Number of transformer blocks
  dropout: 0.1                      # Dropout rate

# Training Configuration
training:
  max_iters: 5000                  # Maximum training iterations
  batch_size: 16                    # Batch size
  eval_interval: 500                # How often to evaluate
  save_interval: 100               # How often to save periodic checkpoints
  learning_rate: 0.0003             # Initial learning rate
  warmup_steps: 1000                # Warmup steps for LR scheduler
  max_steps: 10000                  # Max steps for LR scheduler (often same as max_iters)
  max_grad_norm: 1.0                # Gradient clipping threshold
  patience: 5                       # Early stopping patience
  device: "cuda"                    # Training device (cuda/cpu)
  load_existing_model: false        # Whether to load a checkpoint
  checkpoint_file: "checkpoint.pt"  # Default checkpoint filename
  optimizer_type: "AdamW"           # Optimizer class name
  weight_decay: 0.01                # Moderate regularization
  beta1: 0.9                        # Standard momentum decay
  beta2: 0.999                      # Standard variance decay
  epsilon: 1e-8                     # Default numerical stability

# Logging and Monitoring
logging:
  log_dir: "runs/"                  # Directory for TensorBoard logs
  model_save_dir: "models/"         # Directory for model checkpoints
  log_level: "DEBUG"                # Logging verbosity (DEBUG, INFO, etc.)
  mlflow_tracking_uri: "http://localhost:5000"  # MLflow server URI
  mlflow_enabled: true                          # Enable MLflow tracking

# MLOps Configuration
mlops:
  distributed: false                      # Enable distributed training (DDP)
  num_workers: 4                          # Number of DataLoader workers
  docker_image: "pytorch/pytorch:latest"  # Base Docker image for containerization
  artifact_store: "s3://my-bucket/models" # Cloud storage for artifacts

# Tokenizer Configuration
tokenizer:
  type: "spmTokenizer"              # Tokenizer class name
  vocab_file: "tok400.vocab"        # Path to vocabulary file (if applicable)